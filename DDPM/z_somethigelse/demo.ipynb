{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demostration of my own DDPM\n",
    "\n",
    "The following code may not be able running in jupter, so,copy them to normal python file to run them and follow the following structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part I : DDPM\n",
    "arxiv:2006.11239,v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'ldm' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n ldm ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# this is my own Implementation of DDPM, utlising pytorch lightning.\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS, OptimizerLRScheduler\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from Dataset.CelebaHQ import get_ldmcelebahq \n",
    "from unet import UNetModel\n",
    "\n",
    "\n",
    "\n",
    "class math_helper:\n",
    "    def __init__(self):\n",
    "        # this class is a collection of functions that are used in the DDPM\n",
    "        pass\n",
    "    \n",
    "    def predict_x_0(self,sqrt_recip_alphas_cumprod,x_t,noise):\n",
    "        \n",
    "        coeff=sqrt_recip_alphas_cumprod\n",
    "        output=coeff*(x_t-coeff*noise)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class DDPM(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.model= UNetModel(\n",
    "        in_channels=3,\n",
    "        model_channels=224,\n",
    "        out_channels=3,\n",
    "        num_res_blocks=2,\n",
    "        attention_resolutions=(8,4,2),\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 3, 4),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        num_classes=None,\n",
    "        use_checkpoint=False,\n",
    "        num_heads=1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,)\n",
    "        # lightning configuration\n",
    "        # logic of this DDPM\n",
    "        # 1. register_schedule: calculate some parameters\n",
    "        # 2. tran_step\n",
    "        self.parameterization = \"eps\"\n",
    "        self.l_simple_weight=1.\n",
    "        self.original_elbo_weight=0.\n",
    "        self.math_helper=math_helper()\n",
    "        self.register_schedule(timesteps=1000)# 计算一些类似于alpha，beta的参数\n",
    "\n",
    "    def register_schedule(self,v_posterior=0, beta_schedule=\"linear\", timesteps=1000, \n",
    "                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
    "        # v_posterior是improved DDPM的一部分，先默认为0。它类似一个权重\n",
    "\n",
    "        betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n",
    "                                       cosine_s=cosine_s)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        self.linear_start = linear_start\n",
    "        self.linear_end = linear_end\n",
    "        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n",
    "\n",
    "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer('betas', to_torch(betas)) # 知道\n",
    "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))# 知道\n",
    "        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))# 约等于t-1\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))# 知道\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))# 知道\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod))) # 知道\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod))) # TODO： 这是什么\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1))) # TODO： 这是什么\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0) # 这里是插值那部分，算x_{t-1}的\n",
    "        posterior_variance = (1 - v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n",
    "                    1. - alphas_cumprod) + v_posterior * betas\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n",
    "        self.register_buffer('posterior_mean_coef1', to_torch(\n",
    "            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n",
    "        self.register_buffer('posterior_mean_coef2', to_torch(\n",
    "            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            lvlb_weights = self.betas ** 2 / (\n",
    "                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n",
    "        elif self.parameterization == \"x0\":\n",
    "            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n",
    "        else:\n",
    "            raise NotImplementedError(\"mu not supported\")\n",
    "        # TODO how to choose this term\n",
    "        lvlb_weights[0] = lvlb_weights[1]\n",
    "        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n",
    "        assert not torch.isnan(self.lvlb_weights).all()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # ldm作者的forward写得及其不合理，算loss的过程其实是training_step的过程，而非\n",
    "        # 不如就不要forward 部分了，反正training_step里面才是真正的forward。而且diffusion也没有传统意义上的forward部分。\n",
    "        # diffusion的forward，可以说是加噪声的过程。\n",
    "        # 不要forward了！\n",
    "    def forward(self,x_t,t):\n",
    "        return self.model(x_t,t)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer=torch.optim.Adam(self.parameters(),lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        # batch : [batch_size,width,height,chanel]\n",
    "        x0=batch['image'].permute(0,3,1,2)\n",
    "        t= torch.randint(0,self.num_timesteps,(x0.shape[0],),device=self.device)\n",
    "        noise=torch.randn_like(x0)\n",
    "        \n",
    "        batch_size=x0.shape[0]\n",
    "   \n",
    "        x_t=self.sqrt_alphas_cumprod[t].view(batch_size,1,1,1) * x0+self.sqrt_one_minus_alphas_cumprod[t].view(batch_size,1,1,1)*noise\n",
    "        \n",
    "        model_output=self.model(x_t,t)\n",
    "        \n",
    "        target=noise\n",
    "        \n",
    "        # get loss\n",
    "        loss_dict={}\n",
    "        loss=torch.nn.functional.mse_loss(target,model_output)\n",
    "        \n",
    "        \n",
    "        ## TODO： 以下好像是Improved DDPM的内容\n",
    "        log_prefix = 'train' if self.training else 'val'\n",
    "\n",
    "        loss_dict.update({f'{log_prefix}/loss_simple': loss.mean()})\n",
    "        loss_simple = loss.mean() * self.l_simple_weight\n",
    "\n",
    "        loss_vlb = (self.lvlb_weights[t] * loss).mean()\n",
    "        loss_dict.update({f'{log_prefix}/loss_vlb': loss_vlb})\n",
    "\n",
    "        loss = loss_simple + self.original_elbo_weight * loss_vlb\n",
    "\n",
    "        loss_dict.update({f'{log_prefix}/loss': loss})\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,val_batch,batch_ix):\n",
    "        pass\n",
    "    \n",
    "    # lightningModel hooks, lightningmodules has +20 hooks to keep all the flexibility\n",
    "    def p_sample(self):\n",
    "        x_t=torch.randn(3,256,256,device=self.device)\n",
    "        \n",
    "        for t in reversed(range(1,self.num_timesteps)):\n",
    "            z=torch.randn(3,256,256,device=self.device)\n",
    "           \n",
    "            noise=self.model(x_t,t)\n",
    "\n",
    "            x_0_predicted=self.math_helper.predict_x_0(self.sqrt_recip_alphas_cumprod[t],x_t,noise)\n",
    "            \n",
    "            x_tm1=(self.posterior_mean_coef1[t] *x_0_predicted*self.posterior_mean_coef2[t]*x_t)+self.posterior_variance[t]*z\n",
    "            \n",
    "            x_t=x_tm1\n",
    "        img=x_tm1\n",
    "        \n",
    "        return img\n",
    "        \n",
    "\n",
    "\n",
    "def make_beta_schedule(schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
    "    if schedule == \"linear\":\n",
    "        betas = (\n",
    "                torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n",
    "        )\n",
    "\n",
    "    elif schedule == \"cosine\":\n",
    "        timesteps = (\n",
    "                torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s\n",
    "        )\n",
    "        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n",
    "        alphas = torch.cos(alphas).pow(2)\n",
    "        alphas = alphas / alphas[0]\n",
    "        betas = 1 - alphas[1:] / alphas[:-1]\n",
    "        betas = np.clip(betas, a_min=0, a_max=0.999)\n",
    "\n",
    "    elif schedule == \"sqrt_linear\":\n",
    "        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64)\n",
    "    elif schedule == \"sqrt\":\n",
    "        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64) ** 0.5\n",
    "    else:\n",
    "        raise ValueError(f\"schedule '{schedule}' unknown.\")\n",
    "    return betas.numpy()\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='saved_models/',\n",
    "    filename='model',\n",
    "\n",
    "    save_top_k=1,  # Save the best model based on validation loss\n",
    "    mode='min',     # 'min' or 'max' depending on the metric being monitored\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "test=DDPM()\n",
    "trainer=pl.Trainer(devices=1,max_epochs=5,callbacks=[checkpoint_callback],limit_val_batches=10)\n",
    "train_loader,val_loader=get_ldmcelebahq(batch_size=3)\n",
    "trainer.fit(test,train_dataloaders=train_loader,val_dataloaders=val_loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
