{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Main.py\n",
    "\n",
    "copy it to another .py file and then run it. this will save around 30% time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training Hyperparameters\n",
    "device=torch.device('cuda')\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "\n",
    "# Define the Generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, img_shape),\n",
    "            nn.Tanh()  # Tanh activation for images in the range [-1, 1]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, z): # z means latent variables\n",
    "        img = self.fc(z)\n",
    "        return img\n",
    "\n",
    "# Define the Discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(img_shape, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()  # Sigmoid activation for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        validity = self.fc(img)\n",
    "        return validity\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "img_shape = 28 * 28  # Assuming MNIST-like images\n",
    "\n",
    "# Create the generator and discriminator\n",
    "generator = Generator(latent_dim, img_shape).to(device)\n",
    "discriminator = Discriminator(img_shape).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "\n",
    "\n",
    "# Load a dataset (e.g., MNIST)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for i, (real_imgs, real_labels) in enumerate(dataloader):\n",
    "        real_imgs=real_imgs.to(device)\n",
    "        real_labels=real_labels.to(device)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        batch_size = real_imgs.size(0)\n",
    "        valid = torch.ones((batch_size, 1),device=device)\n",
    "        fake = torch.zeros((batch_size, 1),device=device)\n",
    "\n",
    "        # Generate a batch of random noise\n",
    "        z = torch.randn((batch_size, latent_dim),device=device)\n",
    "\n",
    "        # Generate fake images\n",
    "        fake_imgs = generator(z)\n",
    "\n",
    "        # Train the discriminator：尽可能分辨真伪\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # BCELoss(p,y) = -[y * log(p) + (1 - y) * log(1 - p)], \n",
    "        # when y=1, BCELoss=-log(p),\n",
    "        # when y=0, BCELoss=-log(1-p).\n",
    "        # BCELoss(D(x),1)+BCELoss(D(G(z)),0)=-(log(D(x))+log(1-D(G(z))))\n",
    "        # Here, our objective function is log D ( x^(i)) + log ( 1−D ( G ( z^(i) )))\n",
    "        real_loss = F.binary_cross_entropy(discriminator(real_imgs.view(real_imgs.size(0), -1)), valid)\n",
    "        fake_loss = F.binary_cross_entropy(discriminator(fake_imgs.detach().view(fake_imgs.size(0), -1)), fake)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train the generator ：尽可能骗过判别器\n",
    "        optimizer_G.zero_grad()\n",
    "        # 此时y=1，BCELoss=-log(D(G(z)),此时，D越把G(z)当作真的，BCELoss的绝对值越小。\n",
    "        # 值得注意的是，文中的优化公式是min log(1-D(G(z)))。其实我们不需要管优化公式是什么，\n",
    "        # 是先有为了让D(G(z))越等于1越好，才构建了min(log(1-D(G(z))))这个优化公式。\n",
    "        # 不用min(log(1-D(G(z))))，用这里的-log(D(G(z)))也是可以的。\n",
    "        # 只要目的是让D(G(z))越等于1 就行了。\n",
    "        \n",
    "        \n",
    "        # 总结一下：在设计loss公式的时候，我们先明确让哪个参数趋近于什么数值，然后再构建loss公式。\n",
    "        # 像BCE这个公式，原本意思是交叉熵，这里用得到交叉熵吗？与交叉熵有半毛钱关系？没有！仅仅为了让D(G(z))越等于1越好，才用了BCE这个公式。\n",
    "        g_loss = F.binary_cross_entropy(discriminator(fake_imgs.view(fake_imgs.size(0), -1)), valid) # \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Print progress\n",
    "        if i%100==0:\n",
    "            print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
    "\n",
    "\n",
    "# Save the generator model\n",
    "torch.save(generator, 'generator.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, img_shape),\n",
    "            nn.Tanh()  # Tanh activation for images in the range [-1, 1]\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, z): # z means latent variables\n",
    "        img = self.fc(z)\n",
    "        return img\n",
    "\n",
    "\n",
    "latent_dim=100\n",
    "device=torch.device('cpu')\n",
    "G=torch.load('generator.pth').to(device)\n",
    "z=torch.randn(latent_dim).to(device)\n",
    "\n",
    "print(z)\n",
    "image=G(z).detach().numpy().reshape((28,28))\n",
    "\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment \n",
    "(Wrote in my thinking language (first language), use [chatgpt](https://chat.openai.com) to translate if you want to know what they mean.\n",
    "\n",
    "在GAN这篇文章里面，作者提出了Algorithm 1 Minibatch stochastic gradient descent training of generative adversarial nets.原文如下  \n",
    ">```\n",
    ">for number of training iterations do\n",
    ">  for k steps do  \n",
    ">  ```\n",
    ">  - Sample minibatch of m noise samples $\\{ z^{(1)}, . . . , z^{(m)}\\}$ from noise prior $p_g(z)$.  \n",
    ">  - Sample minibatch of m examples $\\{x^{(1)}, . . . , x^{(m)}\\}$ from data generating distribution $p_{data}(x)$.  \n",
    ">  - Update the discriminator by ascending its stochastic gradient(SGA): \n",
    ">  $$ \n",
    ">  \\nabla_{θ_d}\\frac{1}m \\sum_{i=1}^{m} [ log D ( x^{(i)}) + log ( 1−D ( G ( z^{(i)} )))] \n",
    ">  $$  \n",
    ">```\n",
    ">  end for\n",
    ">```\n",
    "> \n",
    ">  - Sample minibatch of m noise samples $\\{z^{(1)}, . . . , z^{(m)}\\}$ from noise prior $p_g(z)$. \n",
    ">  - Update the generator by descending its stochastic gradient(SGD): \n",
    ">  $$\n",
    ">  \\nabla_{\\theta_g} \\frac{1}m \\sum_{i=1}^m log ( 1−D ( G ( z^{(i)})))\n",
    ">  $$\n",
    ">  ```\n",
    ">  end for\n",
    ">  ```\n",
    ">  The gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiments.   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIAN： The author applied a SDA to discriminator while a SDG to generator!  \n",
    "SGA:  \n",
    "第一项,D(x),x是从dataset中拿出来的，我们需要D(x)越=1越好，此时项越接近0\n",
    "第二项，G（z）为从噪音中生成的图片，我们需要D(G(z))越=0越好，此时项越接近0\n",
    "这么看来,这也应该是求SGD阿，为什么是SGA？\n",
    "作者的说法是错误的，应该还是descending，只是BCELoss是负数，objective function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "在main.py中，首先定义两个MLP，一个是generator，一个是discriminator。对应GAN这篇文章的G & D。\n",
    "训练时：\n",
    "数据准备，从dataloader中拿一个batch的数据来，分别为real_images和real_labels  \n",
    "random出 64个 z出来，作为latent variable。\n",
    "然后将这64个z放入Generator中去，获得fake_images。\n",
    "然后训练用一半real,一半fake来训练discriminator\n",
    "然后用fake来训练generator\n",
    "over\n",
    "\n",
    "\n",
    "\n",
    "$$\\underset{G}{\\min}\\underset{D}{\\max}V(D, G) = E_{x∼p_{data}}(x)[log D(x)] + E_{z∼p_z}(z) [log(1 − D(G(z)))]$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Question:  \n",
    "Why update the D ascending in Generative Adversarial networks?\n",
    "\n",
    "## Answer:\n",
    "Consider this objective function:\n",
    "$$\\log D ( x^{(i)}) + \\log ( 1−D ( G ( z^{(i)} )))$$\n",
    "\n",
    "The first term requires $D$ to correctly recognise real data. The closer $D(x)$ is to 1 (True), the closer the term $\\log D ( x^{(i)})$ is to 0. And this term is always negative or 0. Similarly, for the second term $\\log ( 1−D ( G ( z^{(i)} )))$, situation is the same. They all requires the objective function as closer to zero as possible, which in numeral is ascending the objective function to zero.\n",
    "\n",
    "The description here is misleading, as we usually refer 'stochastic gradient descending (SGD)' as a method for weight updating but not to describe the actual value of objective functions.\n",
    "\n",
    "The sentences \n",
    ">'Update the discriminator by **ascending** its stochastic gradient'  \n",
    ">'Update the generator by **descending** its stochastic gradient' \n",
    "\n",
    "should be replaced with  \n",
    "\n",
    ">'Update the discriminator with an optimiser for **ascending the objective \n",
    "function's value**'  \n",
    ">'Update the generator with an optimiser for **descending the objective \n",
    "function's value**'  \n",
    "\n",
    "if we must use 'ascending' and 'descending' here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The end of original GAN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
